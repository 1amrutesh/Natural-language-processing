{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T17:04:47.734205Z",
     "iopub.status.busy": "2022-05-29T17:04:47.733814Z",
     "iopub.status.idle": "2022-05-29T17:04:56.569760Z",
     "shell.execute_reply": "2022-05-29T17:04:56.568473Z",
     "shell.execute_reply.started": "2022-05-29T17:04:47.734175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (2.3.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.96)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.53)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.23.7)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from boto3->transformers) (0.5.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->transformers) (1.0.0)\n",
      "Requirement already satisfied: botocore<1.27.0,>=1.26.7 in /opt/conda/lib/python3.7/site-packages (from boto3->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.27.0,>=1.26.7->boto3->transformers) (2.8.2)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->sacremoses->transformers) (4.11.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers) (4.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers) (3.8.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers == 2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T17:05:26.979024Z",
     "iopub.status.busy": "2022-05-29T17:05:26.978557Z",
     "iopub.status.idle": "2022-05-29T17:05:27.000305Z",
     "shell.execute_reply": "2022-05-29T17:05:26.999590Z",
     "shell.execute_reply.started": "2022-05-29T17:05:26.978986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# data processing\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import nltk\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# transformers\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TFBertModel\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import TFRobertaModel\n",
    "\n",
    "# keras\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# set seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "# set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine()\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\",\n",
    "       titleweight=\"bold\", titlepad=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T16:57:07.824567Z",
     "iopub.status.busy": "2022-05-29T16:57:07.82394Z",
     "iopub.status.idle": "2022-05-29T16:57:20.745514Z",
     "shell.execute_reply": "2022-05-29T16:57:20.744521Z",
     "shell.execute_reply.started": "2022-05-29T16:57:07.824462Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T17:05:34.333553Z",
     "iopub.status.busy": "2022-05-29T17:05:34.333051Z",
     "iopub.status.idle": "2022-05-29T17:05:34.344893Z",
     "shell.execute_reply": "2022-05-29T17:05:34.343821Z",
     "shell.execute_reply.started": "2022-05-29T17:05:34.333504Z"
    }
   },
   "outputs": [],
   "source": [
    "train_path = '../input/jigsaw-toxic-comment-classification-challenge/train.csv'\n",
    "test_path = '../input/jigsaw-toxic-comment-classification-challenge/test.csv'\n",
    "test_labels_path = '../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv'\n",
    "subm_path = '../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T17:05:38.094707Z",
     "iopub.status.busy": "2022-05-29T17:05:38.094348Z",
     "iopub.status.idle": "2022-05-29T17:05:41.324413Z",
     "shell.execute_reply": "2022-05-29T17:05:41.323638Z",
     "shell.execute_reply.started": "2022-05-29T17:05:38.094678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene',\n",
    "              'threat', 'insult', 'identity_hate']\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "df_test_labels = pd.read_csv(test_labels_path)\n",
    "df_test_labels = df_test_labels.set_index('id')\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T17:07:20.662215Z",
     "iopub.status.busy": "2022-05-29T17:07:20.661871Z",
     "iopub.status.idle": "2022-05-29T17:07:20.666607Z",
     "shell.execute_reply": "2022-05-29T17:07:20.665834Z",
     "shell.execute_reply.started": "2022-05-29T17:07:20.662188Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T17:07:23.210158Z",
     "iopub.status.busy": "2022-05-29T17:07:23.209378Z",
     "iopub.status.idle": "2022-05-29T17:13:38.847627Z",
     "shell.execute_reply": "2022-05-29T17:13:38.846815Z",
     "shell.execute_reply.started": "2022-05-29T17:07:23.210120Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b5d3dc13e34c5f9b995010a500802f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\n",
    "MAX_LEN = 128\n",
    "\n",
    "\n",
    "def tokenize_sentences(sentences, tokenizer, max_seq_len=128):\n",
    "    tokenized_sentences = []\n",
    "\n",
    "    for sentence in tqdm(sentences):\n",
    "        tokenized_sentence = tokenizer.encode(\n",
    "            sentence,                  # Sentence to encode.\n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            max_length=max_seq_len,  # Truncate all sentences.\n",
    "        )\n",
    "\n",
    "        tokenized_sentences.append(tokenized_sentence)\n",
    "\n",
    "    return tokenized_sentences\n",
    "\n",
    "\n",
    "def create_attention_masks(tokenized_and_padded_sentences):\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in tokenized_and_padded_sentences:\n",
    "        att_mask = [int(token_id > 0) for token_id in sentence]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    return np.asarray(attention_masks)\n",
    "\n",
    "\n",
    "input_ids = tokenize_sentences(df_train['comment_text'], tokenizer, MAX_LEN)\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN,\n",
    "                          dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "attention_masks = create_attention_masks(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T17:14:17.415641Z",
     "iopub.status.busy": "2022-05-29T17:14:17.415284Z",
     "iopub.status.idle": "2022-05-29T17:14:17.578543Z",
     "shell.execute_reply": "2022-05-29T17:14:17.577743Z",
     "shell.execute_reply.started": "2022-05-29T17:14:17.415611Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels = df_train[label_cols].values\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n",
    "    input_ids, labels, random_state=0, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(\n",
    "    attention_masks, labels, random_state=0, test_size=0.1)\n",
    "\n",
    "train_size = len(train_inputs)\n",
    "validation_size = len(validation_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T17:14:54.625633Z",
     "iopub.status.busy": "2022-05-29T17:14:54.624844Z",
     "iopub.status.idle": "2022-05-29T17:15:00.320331Z",
     "shell.execute_reply": "2022-05-29T17:15:00.319494Z",
     "shell.execute_reply.started": "2022-05-29T17:14:54.625595Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 17:14:54.690849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-29 17:14:54.691995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-29 17:14:54.692694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-29 17:14:54.695781: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-29 17:14:54.696115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-29 17:14:54.697153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-29 17:14:54.698116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-29 17:14:59.404028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-29 17:14:59.404886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-29 17:14:59.405569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-29 17:14:59.406199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15047 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "NR_EPOCHS = 1\n",
    "\n",
    "\n",
    "def create_dataset(data_tuple, epochs=1, batch_size=32, buffer_size=10000, train=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data_tuple)\n",
    "    if train:\n",
    "        dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "    dataset = dataset.repeat(epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    if train:\n",
    "        dataset = dataset.prefetch(1)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_dataset = create_dataset(\n",
    "    (train_inputs, train_masks, train_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)\n",
    "validation_dataset = create_dataset(\n",
    "    (validation_inputs, validation_masks, validation_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T17:15:37.089535Z",
     "iopub.status.busy": "2022-05-29T17:15:37.089182Z",
     "iopub.status.idle": "2022-05-29T17:15:51.501909Z",
     "shell.execute_reply": "2022-05-29T17:15:51.500917Z",
     "shell.execute_reply.started": "2022-05-29T17:15:37.089505Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "\n",
    "class BertClassifier(tf.keras.Model):\n",
    "    def __init__(self, bert: TFBertModel, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.classifier = Dense(num_classes, activation='sigmoid')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids,\n",
    "                            head_mask=head_mask)\n",
    "        cls_output = outputs[1]\n",
    "        cls_output = self.classifier(cls_output)\n",
    "\n",
    "        return cls_output\n",
    "\n",
    "\n",
    "model = BertClassifier(TFBertModel.from_pretrained(\n",
    "    bert_model_name), len(label_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T17:16:54.326364Z",
     "iopub.status.busy": "2022-05-29T17:16:54.325987Z",
     "iopub.status.idle": "2022-05-29T17:51:55.882531Z",
     "shell.execute_reply": "2022-05-29T17:51:55.881808Z",
     "shell.execute_reply.started": "2022-05-29T17:16:54.326332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== EPOCH 0 ==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93bef9662817468787504afd8b7bf357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 17:17:04.594805: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Step: 0, Loss: 1.0814101696014404\n",
      "toxic roc_auc 0.3620690107345581\n",
      "severe_toxic roc_auc 0.0\n",
      "obscene roc_auc 0.5967742204666138\n",
      "threat roc_auc 0.0\n",
      "insult roc_auc 0.25806450843811035\n",
      "identity_hate roc_auc 0.0\n",
      "\n",
      "Train Step: 1000, Loss: 0.17369689047336578\n",
      "toxic roc_auc 0.893224835395813\n",
      "severe_toxic roc_auc 0.8494138717651367\n",
      "obscene roc_auc 0.9023731350898743\n",
      "threat roc_auc 0.7587840557098389\n",
      "insult roc_auc 0.8770831823348999\n",
      "identity_hate roc_auc 0.8068110942840576\n",
      "\n",
      "Train Step: 2000, Loss: 0.10797756165266037\n",
      "toxic roc_auc 0.980899453163147\n",
      "severe_toxic roc_auc 0.9900580644607544\n",
      "obscene roc_auc 0.9899753332138062\n",
      "threat roc_auc 0.9571836590766907\n",
      "insult roc_auc 0.9845350384712219\n",
      "identity_hate roc_auc 0.973520040512085\n",
      "\n",
      "Train Step: 3000, Loss: 0.08509143441915512\n",
      "toxic roc_auc 0.9845840334892273\n",
      "severe_toxic roc_auc 0.9880608320236206\n",
      "obscene roc_auc 0.9901912212371826\n",
      "threat roc_auc 0.9808844923973083\n",
      "insult roc_auc 0.9836523532867432\n",
      "identity_hate roc_auc 0.9810394048690796\n",
      "\n",
      "Train Step: 4000, Loss: 0.07320581376552582\n",
      "toxic roc_auc 0.9854637384414673\n",
      "severe_toxic roc_auc 0.9882652163505554\n",
      "obscene roc_auc 0.9914578795433044\n",
      "threat roc_auc 0.985634446144104\n",
      "insult roc_auc 0.9887568950653076\n",
      "identity_hate roc_auc 0.9829958081245422\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618ab00a207d45979a3ea04e03989c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, Validation Loss: 0.03755052015185356, Time: 2101.4677963256836\n",
      "\n",
      "toxic roc_auc 0.9871336817741394\n",
      "severe_toxic roc_auc 0.9881709814071655\n",
      "obscene roc_auc 0.9919025301933289\n",
      "threat roc_auc 0.9717094898223877\n",
      "insult roc_auc 0.985230028629303\n",
      "identity_hate roc_auc 0.9688275456428528\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import create_optimizer\n",
    "\n",
    "steps_per_epoch = train_size // BATCH_SIZE\n",
    "validation_steps = validation_size // BATCH_SIZE\n",
    "\n",
    "# | Loss Function\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "validation_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "\n",
    "# | Optimizer (with 1-cycle-policy)\n",
    "warmup_steps = steps_per_epoch // 3\n",
    "total_steps = steps_per_epoch * NR_EPOCHS - warmup_steps\n",
    "optimizer = create_optimizer(\n",
    "    init_lr=2e-5, num_train_steps=total_steps, num_warmup_steps=warmup_steps)\n",
    "\n",
    "# | Metrics\n",
    "train_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\n",
    "validation_auc_metrics = [tf.keras.metrics.AUC()\n",
    "                          for i in range(len(label_cols))]\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, token_ids, masks, labels):\n",
    "    labels = tf.dtypes.cast(labels, tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(token_ids, attention_mask=masks)\n",
    "        loss = loss_object(labels, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables), 1.0)\n",
    "\n",
    "    train_loss(loss)\n",
    "\n",
    "    for i, auc in enumerate(train_auc_metrics):\n",
    "        auc.update_state(labels[:, i], predictions[:, i])\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def validation_step(model, token_ids, masks, labels):\n",
    "    labels = tf.dtypes.cast(labels, tf.float32)\n",
    "\n",
    "    predictions = model(token_ids, attention_mask=masks, training=False)\n",
    "    v_loss = loss_object(labels, predictions)\n",
    "\n",
    "    validation_loss(v_loss)\n",
    "    for i, auc in enumerate(validation_auc_metrics):\n",
    "        auc.update_state(labels[:, i], predictions[:, i])\n",
    "\n",
    "\n",
    "def train(model, train_dataset, val_dataset, train_steps_per_epoch, val_steps_per_epoch, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        print('=' * 50, f\"EPOCH {epoch}\", '=' * 50)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        for i, (token_ids, masks, labels) in enumerate(tqdm(train_dataset, total=train_steps_per_epoch)):\n",
    "            train_step(model, token_ids, masks, labels)\n",
    "            if i % 1000 == 0:\n",
    "                print(f'\\nTrain Step: {i}, Loss: {train_loss.result()}')\n",
    "                for i, label_name in enumerate(label_cols):\n",
    "                    print(\n",
    "                        f\"{label_name} roc_auc {train_auc_metrics[i].result()}\")\n",
    "                    train_auc_metrics[i].reset_states()\n",
    "\n",
    "        for i, (token_ids, masks, labels) in enumerate(tqdm(val_dataset, total=val_steps_per_epoch)):\n",
    "            validation_step(model, token_ids, masks, labels)\n",
    "\n",
    "        print(\n",
    "            f'\\nEpoch {epoch+1}, Validation Loss: {validation_loss.result()}, Time: {time.time()-start}\\n')\n",
    "\n",
    "        for i, label_name in enumerate(label_cols):\n",
    "            print(f\"{label_name} roc_auc {validation_auc_metrics[i].result()}\")\n",
    "            validation_auc_metrics[i].reset_states()\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "train(model, train_dataset, validation_dataset, train_steps_per_epoch=steps_per_epoch,\n",
    "      val_steps_per_epoch=validation_steps, epochs=NR_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
